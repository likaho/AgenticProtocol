version: '3.4'

services:
  llm-server:
    container_name: llm-server
    image: llm-server
    build:
      context: llm-server
      dockerfile: ./Dockerfile
    env_file:
      - ./llm-server/.env
    environment:
      NODE_ENV: production
    ports:
      - 3001:3001

  ai-app-builder:
    container_name: ai-app-builder
    image: ai-app-builder
    build:
      context: ai-app-builder
      dockerfile: ./Dockerfile
    environment:
      - VECTOR_SEARCH_SERVICE_URL=http://llm-proxy-server:8080/api/v1
    ports:
      - 3031:3031

  llm-proxy-server:
    container_name: llm-proxy-server
    image: llm-proxy-server
    build:
      context: llm-proxy-server
      dockerfile: ./Dockerfile
    env_file:
      - ./llm-proxy-server/.env
    environment:
      - GALADRIEL_URL=http://llm-server:3001
      - MARKETPLACE_URL=http://ai-app-builder:3031/api/v1/prediction
    # depends_on:
    #   llm-server:
    #     condition: service_healthy
    #   ai-app-builder:
    #     condition: service_healthy
    ports:
      - 8080:5000

  chat-ui:
    container_name: chat-ui
    image: chat-ui
    build:
      context: chat-ui
      dockerfile: ./Dockerfile
    depends_on:
      llm-proxy-server:
        condition: service_healthy
    ports:
      - 3000:3000

