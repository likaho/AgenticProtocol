version: "3.8"
services:
  llm-server:
    container_name: llm-server
    image: llm-server
    build:
      context: llm-server
      dockerfile: ./Dockerfile
    env_file:
      - ./llm-server/.env
    environment:
      NODE_ENV: production
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3001/api/v1/heartbeat"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 2s
    ports:
      - 3001:3001

  ai-app-builder:
    container_name: ai-app-builder
    image: ai-app-builder
    build:
      context: ai-app-builder
      dockerfile: ./Dockerfile
    environment:
      - VECTOR_SEARCH_SERVICE_URL=http://llm-proxy-server:8080/api/v1
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3031/api/v1/ping"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 40s
    ports:
      - 3031:3031

  llm-proxy-server:
    container_name: llm-proxy-server
    image: llm-proxy-server
    build:
      context: llm-proxy-server
      dockerfile: ./Dockerfile
    env_file:
      - ./llm-proxy-server/.env
    environment:
      - GALADRIEL_URL=http://llm-server:3001
      - MARKETPLACE_URL=http://ai-app-builder:3031/api/v1/prediction
      - CHROMADB_HOST=chroma
      - CHROMADB_PORT=8000

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/api/v1/heartbeat"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 2s
    depends_on:
      llm-server:
        condition: service_healthy
      ai-app-builder:
        condition: service_healthy
      chroma:
        condition: service_healthy
    ports:
      - 8080:5000

  chat-ui:
    container_name: chat-ui
    image: chat-ui
    build:
      context: chat-ui
      dockerfile: ./Dockerfile
    depends_on:
      llm-proxy-server:
        condition: service_healthy
    entrypoint: ["/bin/sh", "-c"] 
    command: ["curl --location 'http://llm-proxy-server:5000/api/v1/prediction/123' --header 'Content-Type: application/json' --data '{\"question\": \"Where is the capital city of Spain?\", \"chatId\": \"123\"}'; npm start"]
    ports:
      - 3000:3000


  chroma:
    container_name: chroma
    image: ghcr.io/chroma-core/chroma:latest
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/heartbeat"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 2s
    volumes:
      - index_data:/chroma/.chroma/index
    ports:
      - 8000:8000

volumes:
  index_data:
    driver: local
  backups:
    driver: local